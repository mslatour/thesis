%%%%%%%%%%%%%%%%%%%%%
% Chapter: Approach %
%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
	\item In this thesis the genetic algorithm was chosen as method to solve
		the task.
	\item Several other method categories are common in the literature and one
		of those, using markov decision processes, is partially worked out by
		the author of this thesis. A comparison of the chosen method with
		these alternatives is given in section~\ref{approach_discussion}
	\item First, section~\ref{approach_genetic_algorithm} describes how
		the task was modelled to be applicable to a genetic algorithm.
\end{itemize}
\section{The Genetic Algorithm approach}
\label{approach_genetic_algorithm}
\begin{figure}[ht!]
	\begin{framed}
		\begin{enumerate}
			\item Initialization
			\item Evaluation of each candidate
			\item Repeat until termination condition is satisfied:
				\begin{enumerate}
					\item Parent selection
					\item Recombination of parent pairs
					\item Mutation of the resulting offspring
					\item Evaluation of each candidate
					\item Survivor selection
				\end{enumerate}
		\end{enumerate}
	\end{framed}
	\caption[The evolutionary algorithm]{The general scheme of an
		Evolutionary Algorithm as presented in \citep{Eiben2007}}
	\label{alg:ea_scheme}
\end{figure}
\begin{itemize}
	\item refer to \ref{alg:ea_scheme}.
	\item refer to \citep{Eiben2007} for listing the important aspects of any evolutionary algorithm that need to be modeled: representation, fitness, population, parent selection, variation operators, survivor selection, initialization and termination conditions
\end{itemize}
\subsection{Encoding}
\subsubsection{The resource gene}
\begin{itemize}
	\item Each LO is encoded as a single gene
	\item The encoding is based on the unique identifier
\end{itemize}
\subsubsection{The group gene}
\begin{figure}[ht!]
	\centering
	\includegraphics[scale=0.7]{images/group_dimensions.pdf}
	\caption[Group characterization]{Students are characterized in four groups.
		The horizontal axis represents the pre-test score, the vertical axis
		the relevant prior experience.}
	\label{fig:group_dimensions}
\end{figure}
\begin{itemize}
	\item Each student is characterized by two dimensions
		\begin{itemize}
			\item score on the pre-test for this particular knowledge component
			\item relevant experiences
		\end{itemize}
	\item Other things that could have been included are learning
		style, gender and age. As is also common in most user models in
		curriculum sequencing applications. However, in this thesis it
		was decided not to include those. Partly because of the
		difficult way some of the indicators can be retrieved or
		checked automatically, as is for example difficult for learning
		style. And partly because it would enlarge the state space even
		more.
	\item The two dimensions are discretized into two values, high and low.
	\item This results in four categories:
		\begin{description}
			\item[A] This person has a lot of relevant experience but did not
				score well on this particular knowledge component in the
				pre-test.
			\item[B] This person has a lot of relevant experience and also
				scored high on the pre-test for this knowledge component.
			\item[C] This person has neither much relevant experience nor
				a high score on the pre-test for this knowledge component.
			\item[D] This person does not have a lot of relevant experience,
				but did score high on the pre-test on this knowledge component.
		\end{description}
	\item The four categories are also depicted in figure~\ref{fig:group_dimensions}.
	\item These categories are encoded in a gene. This gene functions as a
		match maker. A chromosome containing a particular group gene can only
		be presented to a student that falls in that category.
\end{itemize}
\subsubsection{The sequence chromosome}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{images/concept_hierarchy2.pdf}
	\caption[Components of the chromosome]{Components of the chromosome}
	\label{fig:chromosome_components}
\end{figure}
\begin{itemize}
	\item Refer to Figure~\ref{fig:chromosome_components}.
	\item The chromosome contains exactly one group gene
	\item The chromosome contains one or more resource genes.
	\item There should be a bias towards smaller chromosomes.
\end{itemize}
\subsection{Fitness function}
\begin{itemize}
	\item evaluation function and fitness function are not by definition
		interchangeable. The evaluation function, or objective function,
		provides a measure of performance. The fitness function transforms this
		into reproductive opportunities. Unline the evaluation function, the
		fitness function is defined with respect to the fitness of the other
		members in the population. From: \citet{Whitley1994}. (Paraphrase
		this!) In other words, the fitness function is at least a normalized
		version of the evaluation function obtained by dividing by the average
		fitness value of the current population/generation.
	\item The literature contains many different implementations of fitness
		functions for the curriculum sequencing problem. 
	\item Some solutions include a term that expresses how well a particuluar
		solution fits the pre-determined prerequisite structure of the learning
		objects. Others include a term that expresses how smooth the transitions
		are between learning objects in difficulty.
	\item A big part of the prerequisite structure is already captured by the
		given main curriculum in this thesis that provides the order of
		knowledge components. The curriculum sequencing step takes place
		in this thesis on a much smaller level where the different learning
		objects all try to convey the same content, but do so in different
		ways. In a way this means that this thesis implements a particular type
		of curriculum sequencing that focuses on the presentation. Some objects
		might contain an example or the formal description. Some might present
		the knowledge in text, others are more visual. Yet it is not expected
		to need only one learning object per student, if you would consider
		the learning object term referring to the smallest unit of explanation.
		Different students require a different mix of presentations at perhaps
		even different orders. Furthermore, learning objects are often not
		a perfect fit. They might explain too much or too little about some
		context. On top of that, it is not that well indexed in terms of the
		exact type of presentation that they have. Thus, what we want is a
		sequence of imperfect learning objects that together maximize the
		educational performance. We do not know what the order should be, given
		that the order is a matter of pedagogy and not knowledge engineering.
		And even if we were able to fully specify the right pedagogical order
		for each type of student perfectly. We would still not have the
		required information about these learning objects, or the information
		might be wrong. In conclusion, we are learning a sequence of black
		boxes of which we only know that they attempt to teach a particular
		knowledge component, or so they say.
	\item The only way we can measure the value of a particular sequence for a
		group of students, and thereby assess its fitness, is to look at the
		gain in knowledge as observed by the post-test.
	\item More precisely the fitness function is the normalized learning gain
		between the pre-test and post-test for a given knowledge component.
	\item The observed fitness is probably not the same each time a chromosome
		is evaluated. This is due to the fact that students are not identical,
		especially not given the coarse division into four groups. A solution
		is to see the fitness as a stochastic variable that has some noise on
		top of the ``true'' value. In order to obtain an estimate of this true
		value, several approaches are possible. The most simple one is to take
		multiple samples and average over them. However, in this case, taking
		samples must be considered to be expensive. The approach taken must
		therefor try to minimize the number of samples while maximizing the
		certainty of the fitness value.
	\item According to \citet{Jin2005}, fitness inheritence is a popular approach for evolutionary approximation. In fitness inheritence a child inherits some of the fitness information of its parents.
		% From: A comprehensive survey of fitness approximation
		%  in evolutionary computation. By: Y. Jin (2005)
		% Chen J-H, Goldberg DE, Ho S-Y, Sastry K (2002) Fitness
		%  inheritance in multi-objective optimization.
		% Sastry K, Goldberg DE, Pelikan M (2001) Don’t evaluate,inherit.
		% Smith R, Dike B, Stegmann S (1995) Fitness inheritance in
		%  genetic algorithms
		% Zhang X, Julstrom B, Cheng W (1997) Design of vector
		%  quantization codebooks using a genetic algorithm
	\item The following components can be put in the fitness function:
		\begin{itemize}
			\item linear combination of fitness values of the two parents that
				had crossover applied to it. The weights could be determined on
				the portion of the parent that ended up in the child and the
				uncertainty of the fitness value of that parent.
			\item the uncertainty of the fitness value based on how many times
				it was observed, the variation of those observations and a
				linear combination of the uncertainty values of the parents
				plus some constant uncertainty bonus.
			\item 
		\end{itemize}
\end{itemize}
\subsection{Population}
\begin{itemize}
	\item A population of individual chromosomes evolves over generations
	\item Size: 10?
	\item Requirements:
		\begin{enumerate}
			\item Each chromosome must be unique in the population, so unlike
				most applications this is not a multi-set. Right?
				I'm doubting about this now:
				\begin{itemize}
					\item The initial population size is probably bigger than
						the gene pool of most knowledge components. That means
						that probably the first population already has to be a
						multiset, or it will be smaller than other generations.
					\item The exploration vs exploitation tradeoff states that
						you should also not focus too much on exploration. If
						you would only go for unique chromosomes in each
						generation than that would be mean that the best
						solution is only presented to one student. The others
						could get suboptimal solutions, even if we know that is
						the case. Of course you need to explore, even
						suboptimal solutions. But maybe this should be more
						stochastic than deterministic.
				\end{itemize}
		\end{enumerate}
	\item After having evaluated each member of a generation, a new generation
		is formed.
	\item Certain chromosomes are kept as is, due to the implementation of elitism
		[1 or 2 I think?]
	\item The other chromosomes have a certain probability of surviving. (see
		section~\ref{sec:approach_survivor_selection}.
	\item From the surviving chromosomes, some parents are selected to produce
		offspiring. See section~\ref{sec:approach_parent_selection}.
	\item The offspring is produced according to a set of combination operators
		on the chromosomes of each parent pair. The combination operators have
		a certain probability to be picked. See
		section~\ref{sec:approach_combination_operator}.
	\item The new offspring and old survivors have a certain probability of
		mutation occuring which results in a slightly different individual. See
		section~\ref{sec:approach_mutation_operator}.
	\item Point to a picture depicting the percentages of each population that
		came from elitism, survival, cross-over, mutation. Vertical bar, split
		up into several sections with different colors.
\end{itemize}
\subsection{Parent selection}
\label{sec:approach_parent_selection}
\begin{itemize}
	\item Individuals are selected as parents with a probability proportional
		to their fitness.
\end{itemize}
\subsection{Combination operator}
\label{sec:approach_combination_operator}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{images/crossover_operations.pdf}
	\caption[Crossover operations]{Crossover operations. \textbf{Note:} Image
is still wrong. a) combination results in two children by combining parent 1
with parent 2 and the other way around, b) it makes more sense to count from
the beginning in one parent and from the end in the other than counting from
the beginning in both. Now it is impossible to take the end of parent 2 and
append it to parent 1 and c) maybe this is also not the best way to show,
perhaps just display an example cut point and only the two resulting children}
	\label{fig:crossover_operations}
\end{figure}
\begin{itemize}
	\item Refer to Figure~\ref{fig:crossover_operations} as a enumeration of
		all possible crossover results for a given example of two parents
	\item Describe the crossover operations formally
	\item Describe the probability distribution
\end{itemize}
\subsection{Mutation operator}
\label{sec:approach_mutation_operator}
\begin{itemize}
	\item Perhaps not real mutation, since that is often only referred to as a
		completly random unbiased change. This can result in invalid outcomes.
	\item The mutation, or otherwise unary, operator in this thesis
		(uniformly?) selects a mutation from a set of possible changes that result
		in valid and unique outcomes.
	\item The mutation operators include:
		\begin{enumerate}
			\item Randomly swapping the group gene of the chromosome with a
				different group gene from the pool. In such a way that the
				resulting chromosome is not already present in the population.
			\item Randomly swapping a resource gene of the chromosome with a
				different resource gene from the pool. In such a way that the
				resulting chromosome is not already present in the population.
			\item Randomly swap the positions of two resource genes in the
				chromosome. While still obaying the uniqueness condition of the
				population.
			\item Randomly removing one resource gene from the chromosome.
			\item Randomly adding one resource gene from the pool to the
				chromosome without introducing duplicates
		\end{enumerate}
\end{itemize}
\subsection{Survivor selection}
\label{sec:approach_survivor_selection}
\begin{itemize}
	\item Factors that should influence the probability of a candidate to be a
		survivor are:
		\begin{itemize}
			\item The observed fitness
			\item The uncertainty of the fitness value. More uncertain values
				should get more chance of being selected for a new generation
			\item Some linear combination of the teacher ratings for each
				individual learning objects.
			\item Elitism: the best performing organisms are transfered to the next generation unaltered.
		\end{itemize}
\end{itemize}
\subsection{Initialization}
\begin{itemize}
	\item We start with chromosomes of length 1
	\item The population has 10 chromosomes in each generation
	\item The initial population is not generated entirely randomly, see
		section~\ref{sec:approach_bootstrapping}.
\end{itemize}
\subsubsection{Bootstrapping}
\label{sec:approach_bootstrapping}
\begin{itemize}
	\item The selection is made from the gene pool based on a PDF that reflects
		the teacher ratings
	\item Not all learning objects have teacher ratings
	\item Each rating is an average given the number of raters. This number of raters should have impact on the value of the rating.
	\item Learning objects that don't have received a rating yet, are assumed
		to have a neutral rating. So value 3 on a 5 point scale. Alternatively, if the number of raters is linked to the weight of the rating, the unrated learning objects should just experience no effect of any rating variable (weight equals zero).
	\item Weighted ratings could work even better if the ratings are normalized with an average value of zero, resulting in a symmetrical scale of positive and negative values.
\end{itemize}
\subsection{Termination condition}
\begin{itemize}
	\item We don't want the evolution to stop before all learning objects have
		been given a chance to prove themselves.
	\item One possible termination condition is that when there are no more
		possible candidates to try, and the confidence of the fitness of these
		candidates is high enough to be considered stable. The latter
		requirement is yet to implement theoretically.
	\item Given the growing nature of the gene pool in the presented
		application, it might be possible that this situation never occurs
		while the algorithm is running. Futhermore,	even if that would occur,
		it is possible that after the termination of the program a new learning
		object is added to the set. We don't want everything to start from
		scratch again.
	\item Therefore the algorithm is meant to converge, but never terminate.
\end{itemize}
\section{Discussion}
\label{approach_discussion}
\subsection{Benefits of the approach}
\begin{itemize}
	\item More natural fit to the problem of curriculum sequencing, given the
		natural mapping of chromosomes and genes to the domain.
	\item The method is good for changing environments (citation needed)
	\item No need to explitely specify intermediate states
	\item Domain allows for many enhancements of the canonical algorithm by
		adding prior knowledge in the initialization and fitness function, by
		propagating fitness information to next generations using fitness
		inheritance, and restrict the crossover and mutation operations to
		ensure only valid and interesting outcomes.
	\item Search through the solution space is biased towards areas that seem
		most promising, with a small but nonzero chance of exploring in other
		areas.
	\item This implicit exploration-exploitation trade-off can be enhanced by
		incorperating uncertainty information
\end{itemize}
\subsection{Downsides of the approach}
% risk of Hamming walls
\subsection{Comparing with the Markov Decision Process Approach}
\begin{itemize}
	\item Paraphrase/quote/link from \citep{Whiteson2012} the following points.
		(\textbf{Currently copied literally}, better not keep it that way)
	\item evolutionary methods do not take advantage of the fact the an agent travels
          to states while performing actions. However evolutionary computation
		  is still a popular method for solving reinforcement learning
		  problems. There are three main reasons why.:
		  \begin{enumerate}
			  \item ``First, evolutionary methods can cope well with
					partial observability. While evolutionary methods do not exploit the relationship
					between subsequent states that an agent visits, this can be advantageous when the
					agent is unsure about its state. Since temporal-difference methods rely explicitly
					on the Markov property, their value estimates can diverge when it fails to hold,
					with potentially catastrophic consequences for the performance of the greedy policy.
					In contrast, evolutionary methods do not rely on the Markov property and will always
					select the best policies they can find for the given task. Severe partial
					observability may place a ceiling on the performance of such policies, but
					optimization within the given policy space proceeds normally (Moriarty et al, 1999).
					In addition, representations that use memory to reduce partial observability,
					such as recurrent neural networks, can be be optimized in a natural way with
					evolutionary methods (Gomez and Miikkulainen, 1999; Stanley and Miikkulainen, 2002;
					Gomez and Schmidhuber, 2005a,b).``
				\item ``Second, evolutionary methods can make it easier to find
					suitable representations for the agent’s solution. Since policies need only specify
					an action for each state, instead of the value of each state-action pair, they can be
					simpler to represent. In addition, it is possible to simultaneously evolve a
					suitable policy representation``
				\item ``Third, evolutionary methods provide a simple way to solve
					problems with large or continuous action spaces. Many temporal-difference
					methods are ill-suited to such tasks because they require iterating over the action
					space in each state in order to identify the maximizing action. In contrast,
					evolutionary methods need only evolve policies that directly map states to actions. Of
					course, actor-critic methods (Doya, 2000; Peters and Schaal, 2008) and other techniques
					(Gaskett et al, 1999; Mill ́an et al, 2002; van Hasselt and Wiering, 2007) can
					also be used to make temporal-difference methods suitable for continuous action
					spaces. Nonetheless, evolutionary methods provide a simple, effective way to
					address such difficulties.``
				\item \citet{Whiteson2012} further mentions: ``Of course, none
					of these arguments are unique to evolutionary methods, but
					apply in principle to other policy-search methods too.
					However, evolutionary methods have proven a particularly
					popular way to search policy space and, consequently, there
					is a rich collection of algorithms and results for the
					reinforcement-learning setting. Furthermore, as modern
					methods, such as distribution-based approaches, depart
					further from the original genetic algorithms, their
					resemblance to the process of natural selection has
					decreased. Thus, the distinction between evolutionary
					methods and other policy search approaches has become
					fuzzier and less important.''
		  \end{enumerate}
\end{itemize}
\subsection{Comparing with the Knowledge Engineering Approach}
