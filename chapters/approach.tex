%%%%%%%%%%%%%%%%%%%%%
% Chapter: Approach %
%%%%%%%%%%%%%%%%%%%%%
There are several ways to model the educational task described in
Section~\ref{ch:curriculum_sequencing}. The work presented in this thesis models the
problem using genetic algoritms.

%%%
% General introduction
%%%
Genetic algorithms evolve a population of individuals. An individual contains a
chromosome that represents a candidate solution. A particular chromosome can be contained by
multiple individuals, and as such a candidate solution can be present multiple
times in one generation of the population. Chromosomes are a configuration of
genes, where each gene represents a component of the candidate solution. In the
analogy with natural evolution these genes typically represent certain traits
of the animal, such as the length of wings or the color, that affect the change
of survival. As a result, the configuration of genes can be optimized by inheritence
natural selection. The genetic algorithm works in a similar way, where the
fitness of an individual with a particular configuration of genes determines
the chances of new individuals arising with similar configurations. The
standard outline of this algorithm is given in Figure~\ref{alg:ea_scheme}.
\label{approach_genetic_algorithm}
\begin{figure}[ht!]
	\begin{framed}
		\begin{enumerate}
			\item Initialization
			\item Evaluation of each candidate
			\item Repeat until termination condition is satisfied:
				\begin{enumerate}
					\item Parent selection
					\item Recombination of parent pairs
					\item Mutation of the resulting offspring
					\item Evaluation of each candidate
					\item Survivor selection
				\end{enumerate}
		\end{enumerate}
	\end{framed}
	\caption[The evolutionary algorithm]{The general scheme of an
		Evolutionary Algorithm as presented in \citep{Eiben2007}}
	\label{alg:ea_scheme}
\end{figure}

\citep{Eiben2007} enumerates the important aspects of any
evolutionary algorithm that need to be modeled: representation
(Section~\ref{sec:approach_representation}), initialization
(Section~\ref{sec:approach_initialization}), termination conditions
(Section~\ref{sec:approach_termination}), fitness
(Section~\ref{sec:approach_evaluation}) and parent selection, variation
operators and survivor selection
(Section~\ref{sec:approach_generation_switch}). The applied island model to
support for related student groups is described in
Section~\ref{sec:approach_island_model}.
Finally, Section~\ref{sec:approach_discussion} discusses the advantages and
disadvantages of using genetic algorithms as a model and compares it with the
two alternative modeling choices.


%%%%
% Representation/Domain modeling
%%%
\section{Representation of the domain}
\label{sec:approach_representation}
In this thesis, each educational resource is encoded as a single gene. A
chromosome represents a particular sequence of resources. The order of these
resources in the sequence matter, which is not often the case in genetic
algorithm applications, as it determines in what order material is presented to
the student. An individual denotes a particular teaching attempt where a particular
sequence of the education resources is presented to a student. Each knowledge
component and student group combination is represented by a separate
population. These populations are however not entirely independent, see
Section~\ref{sec:approach_island_model}.

It is not known apriori what the length of the optimal sequence of material is,
which means that chromosomes have variable length. For practical reasons this
length will be bounded to a lower and upper limit in the application. 

Instead of using the standard binary encoding (i.e. the binary representations
of all genes concatenated into one string of bits), a chromosome is encoded as a list
of gene identifiers (i.e. integers). There are two main arguments for this decision.
First, the operations defined in Section~\ref{sec:approach_combination_operator}
and Section~\ref{sec:approach_mutation_operator}
take the educational resource as smallest unit. In applications that use the
standard binary encoding it is common to operate on a bit level, regardless of
the start and finish of each gene representation. Second, the application
created in this thesis which applies this genetic algorithm model is web-based
and uses relational databases to store the chromosomes. The genetic operations
are implemented to work with these database entities and consist largely of
database queries. This combined with the fact that the chromosome has variable
length requires a many-to-many relation between separate chromosome and gene
entities. The alternative of storing a bit string for each chromosome defeats
the purpose of having a database optimized for relational algebra.
% TODO: There should be a bias towards smaller chromosomes.

%%%
% Initialization
%%%
\section{Initialization}
\label{sec:approach_initialization}
The first generation of the population is initialized with a fixed number of
individuals. Each individual contains a chromosome with exactly one gene. The
individuals are generated according to the following steps:
\begin{enumerate}
	\item For all genes in pool:
		\begin{enumerate}
			\item If population is full, stop
			\item Else, add an individual with the chromosome that contains only that gene.
		\end{enumerate}
	\item While there is room left in the population:
		\begin{enumerate}
			\item \label{init_sample_step}Select a gene according to some probability density function
			\item Add an individual with the chromosome that contains only that
				gene
		\end{enumerate}
\end{enumerate}
The probability density function (PDF) refered to in
step~\ref{init_sample_step} is a uniform distribution by default, but can also
represent apriori values of resources as described in
\ref{sec:approach_bootstrapping}.
\subsection{Bootstrapping}
\label{sec:approach_bootstrapping}
% Should this be mentioned, since it has not been used in the experiment?
Some of the OER repositories allow for teachers to give ratings for educational
material. These ratings could be used as an apriori probability that the
material is good. However, only a part the collection of materials will be
rated. Therefore, educational material that does not have a rating is assumed
to be rated neutral (i.e. the middle of a five-point scale).
% Each rating is an average given the number of raters. This number of raters should have impact on the value of the rating.
% Weighted ratings could work even better if the ratings are normalized with an average value of zero, resulting in a symmetrical scale of positive and negative values.

%%%
% Termination
%%%
\section{Termination}
\label{sec:approach_termination}
Given the inherent noise in the fitness values, the algorithm should not stop
before the fitness of each possible\footnote{i.e. all possible sequences given
the constraints on length and the uniqueness requirement} chromosome is determined
with some certainty. That would seem to lead to a valid point of termination
when all chromosomes are tried are evaluated with enough certainty. However,
the pool of genes is assumed to grow (i.e. new educational resources are made
available) and each time a new gene is introduced it theoretically needs to be
tried out in every combination with the already existing genes before the valid
termination point would be reached. This would mean that the algorithm would
never terminate, as it should wait for new genes to arrive. If it is vital that
the algorithm finishes, a practical approach could be to stop if the fitness of
one or more individuals is within a small margin of the optimal value. Provided
an optimal value can be defined.

The application presented in this thesis does not require the genetic algorithm
to terminate. The web-based variation to the algorithm described in
Section~\ref{sec:web-based_ga} ensures that computation only happens on a event
basis. Furthermore, due to the nature of the application, it is not
as interesting to have the best solution at the end as it is to select the best
known solution at each point that an individual is tested. Naturally a
exploration-explotation trade-off applies where occassionally individuals need
to be tried out that could both be better or worse. So instead of termination,
moving towards convergence is important.

%%%
% Candidate evaluation
%%%
\section{Candidate evaluation}
\label{sec:approach_evaluation}
%%%
% 	Fitness function
%%%
\subsection{Fitness function}
% From: A comprehensive survey of fitness approximation
%  in evolutionary computation. By: Y. Jin (2005)
% Chen J-H, Goldberg DE, Ho S-Y, Sastry K (2002) Fitness
%  inheritance in multi-objective optimization.
% Sastry K, Goldberg DE, Pelikan M (2001) Don’t evaluate,inherit.
% Smith R, Dike B, Stegmann S (1995) Fitness inheritance in
%  genetic algorithms
% Zhang X, Julstrom B, Cheng W (1997) Design of vector
%  quantization codebooks using a genetic algorithm
Learning objects are often not a perfect fit. They might explain too much or too
little about some context. On top of that, it is not that well indexed in terms of the
exact type of presentation that they have. Thus, what we want is a
sequence of imperfect learning objects that together maximize the
educational performance. We do not know what the order should be, given
that the order is a matter of pedagogy and not knowledge engineering.
And even if we were able to fully specify the right pedagogical order
for each type of student perfectly. We would still not have the
required information about these learning objects, or the information
might be wrong. In conclusion, we are learning a sequence of black
boxes of which we only know that they attempt to teach a particular
knowledge component.

The only way we can measure the value of a particular sequence for a
group of students, and thereby assess its fitness, is to look at the
gain in knowledge as observed by the post-test.

More precisely the fitness function used in this thesis is the normalized
learning gain between the pre-test and post-test for a given knowledge component,
given by $\frac{C^2 - C^1}{1-C^1}$ where $C^1$ and $C^2$
represent the percentage of correct answers on the pre-test and post-test
respectively of the student.

The observed fitness is probably not the same each time a chromosome
is evaluated. This is due to the fact that students are not identical,
especially not given the coarse division into student groups. A solution
is to see the fitness as a stochastic variable that has some noise on
top of the ``true'' value. In order to obtain an estimate of this true
value, several approaches are possible. The most simple one is to take
multiple samples and average over them. However, in this case, taking
samples must be considered to be expensive. The approach taken must
therefor try to minimize the number of samples while maximizing the
certainty of the fitness value. Which is why Upper Confidence Bound selection
was applied in this thesis, as described in Section~\ref{sec:approach_ucb}.

%%%
%   UCB-1 selection
%%%
\subsection{UCB Selection}
\label{sec:approach_ucb}
The consequence of having a fitness function that involves actual
students is that the event of evaluating a suboptimal individual
also causes damage to the learning process of the student. Something
which is much less the case in a simulation environment.

In the reinforcement learning paradigm this damage is expressed in regret, which
is defined as the difference in performance with the choice made and the
optimal choice. You do however need to evaluate all individuals to see if one
scores better. You even need to evaluate each individual several times in order
to deal with outliers and varience. However you don't want to evaluate an
individual too often when you know it performs suboptimally, given the regret
it will cause. This is a classic exploration vs. exploitation trade-off that is
found in any one-armed bandit problem, where in this application the sequences
of educational material are the one-armed bandits.

In this thesis, the Upper Confidence Bound (UCB) selection algorithm is used to
determine which of the individuals will be evaluated. In
particular the UCB-1\cite{Auer2002} algorithm is used. In UCB-1,
first every individual is evaluated once. After this has been done, the
individual is evaluated for which equation~\eqref{eq:ucb1_value} is maximized,
where $\overline{x_i}$ denotes the average fitness of the individual, $n_i$
denotes the number of times the individual has been evaluated so far and $n$
denotes the overal number of evaluations that occured.

\begin{equation}
	\overline{x_i} + \sqrt{\frac{2 \ln n}{n_i}}
	\label{eq:ucb1_value}
\end{equation}

UCB-1 is proven in \cite{Auer2002} to logarithmically bound the
regret, which ensures that a suboptimal individual is selected logarithmically
less often than the optimal individual. It is important to note that UCB-1 can
only consider the individuals that are present in the current generation of the
population for which the evaluation occurs. This means that there is some
interplay between the UCB-1 mechanism and the selection mechanism of the
genetic algorithm, where the genetic algorithm is responsible for searching
through the solution space efficiently and the UCB-1 algorithm is responsible
for reducing the regret.

%%%
% Generation switch
%%%
\section{Generation switch}
\label{sec:approach_generation_switch}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.8\linewidth]{images/generation_switch.png}
	\caption[Generation switch]{The operations and intermediate phases during a generation switch. Illustration was taken from \cite{Whitley1994}.}
	\label{fig:generation_switch}
\end{figure}
This thesis implements generational replacement with elite preservation, which
are commonly used strategies for survivor selection in the curriculum
sequencing domain \cite{AlMuhaideb2011}.
Elite preservation means that the best performing individuals from the current
generation will transfered directly to the new generation, without any
alteration. Elitism has the purpose of ensuring that cross-over or mutation
operators cannot push the evolution away from what was most promising.
Only when better candidates are found will the elite individual not necessarilly survive.

Generational replacement refers to the strategy where the number of individuals
in the new generation remains the same as in the current generation. The
individuals in the new generation are different individuals\footnote{The new
generation  could still contain the same chromosomes.} than those in the
current generation, with the exception of the elite individuals.

The individuals of the new generation are created in two steps, as is depicted
by Figure~\ref{fig:generation_switch}. First surivors are selected from the
current generation and put in an intermediate generation, which is described in
Section~\ref{sec:approach_survivor_selection}. The survivors are then grouped
into parent pairs, as discussed in
Section~\ref{sec:approach_parent_selection}.
Section~\ref{sec:approach_combination_operator} describes how these parents
then produce offspring by combining the chromosomes of both parents. In
Section~\ref{sec:approach_mutation_operator} the mutation operations that
can occur are described.

%%%
% 	Survivor selection
%%%
\subsection{Survivor selection}
\label{sec:approach_survivor_selection}
In this thesis, survivors are selected from the current generation using the
roulette wheel selection. In this selection strategy, each individual from the
current generation is assigned a portion of a roulette wheel, proportionate to
the normalized fitness of the individual in the current generation. A survivor
is then sampled by uniformly picking a position on the wheel and selecting the
the individual to who that spot on the wheel was assigned to. This is analogous
to spinning the roulette wheel perfectly. The higher the fitness of the
individual, the bigger the area on the roulette wheel and thereby also the more
likely the individual will survive. A individual can be selected multiple
times, with the consequence that some individuals might not survive.

Due to the normalization of the fitness values, the roulette wheel selection
is implemented in three steps. First, take a random number between zero and one.
Second, loop through all individuals until the cumulative fitness is bigger
than the random number. Third, the last individual in the loop is selected as
survivor.

%%%
%  	Parent selection
%%%
\subsection{Parent selection}
\label{sec:approach_parent_selection}
Pairs of individuals are taken from the list of survivors, excluding the elite
individuals, from top to bottom. That means that parents are matched based on
the order that they were selected in the roulette wheel selection described in
Section~\ref{sec:approach_survivor_selection}. The pairing is also shown in
Figure~\ref{fig:generation_switch}. Each parent pair results in offspring
based on the crossover operations described in
Section~\ref{sec:approach_combination_operator}. If the number of survivors is
odd, there will be one suvivor remaining after all pairs have been formed.
This remaining survivor's chromosome is added to the new generation
as its own offspring.

%%%
%  	Crossover operation
%%%
\subsection{Combination operator}
\label{sec:approach_combination_operator}
When two parents are matched to create offspring, their chromosome's are
combined using a crossover operation. The resulting chromosome is placed in a
new individual. There are two crossover operations implemented for this thesis:
\emph{one-point crossover} and \emph{append crossover}.

\paragraph{One-point crossover} The one-point crossover operation is typically
implemented by picking one point for both parents to split, after which the
four halfs are recombined into two new children. The individuals in this
thesis, however, can vary in length. That means that crossover points could be
selected that do not exist in both parents. Naturally one could restrict the
set of valid crossover points to be within the boundaries of both chromosomes.
However, that would also limit valid chromosomes, even though they can be
achieved by combining both parent chromosomes.

In this thesis, the one-point crossover operator is implemented differently.
Instead of picking one point for both parents at once, one crossover point in
each parent is randomly picked independent from the other. These two crossover
points then split up both parents in two pieces each, allowing for the
formation of two new children after recombination. The implementation ensures
that only valid children are the result of the operation. When no valid
children can be created, the one-point crossover is skipped and the append
crossover is attempted.

\paragraph{Append crossover} The append crossover was designed for the edge
case where two parents cannot be split up and recombined into two new valid
children. For example when one or both parents have a chromosome with one gene,
which is impossible to split up. The append crossover operator simply appends
one parent after the other. The two ways to do this result in two children.

%%%
%  	Mutation operation
%%%
\subsection{Mutation operator}
\label{sec:approach_mutation_operator}
Most of the mechanism in genetic algorithms ensures that individuals in each
generation are samples from increasingly smaller areas in the solution
hyperspace. This has the effect that large parts of that hyperspace is never
evaluated. To prevent that from limiting the genetic algorithm to local optima,
the mutation operator is used. After the chromosomes in the offspring are
determined using the crossover operation, each child's chromosome has a small
chance of mutation. In this thesis three different mutation operators have been
applied: \emph{swap mutation}, \emph{addition mutation}, \emph{deletion mutation}.

\paragraph{Swap mutation} The swap mutation operation randomly picks two
distinct genes in a chromosome that are swapped. The chromosome needs to
contain at least two genes in order to be applied to this mutation. If this is
not the case, a different mutation is attempted.

\paragraph{Addition mutation} The chromosome applied to the addition mutation
operator will be appended with a new gene from the gene pool. The gene that is
added must not already exist in the chromosome. If no gene can be selected from
the pool that satisfies this constraint, a different mutation is attempted.

\paragraph{Deletion mutation} The deletion mutation operation deletes a random
gene from the chromosome, resulting in a shift in position of the genes after
it. The resulting chromosome must have at least one gene left. If this is not
possible, a different mutation is attempted.

%%%
% Island model
%%%
\section{Island model}
\label{sec:approach_island_model}
As described in Section~\ref{sec:approach_representation}, the curriculum
sequencing problem is done per knowledge component and per student group. These
are represented as separate populations. However, the populations that
represent the same knowledge component but different student groups actually
co-evolve. The island model was used to model exchange of information between
related populations. After each generation of a population, the best
individuals from the other related populations can migrate towards this
population, replacing the worst individual. The migrated individuals are
copies, and do not change the occurence of the migrated chromosomes in the
originating populations.

Important to note is that all other actions of the genetic algorithm in each
population are still independant, meaning that the populations can also evolve
at different speeds. One reason for this would be that the distributions of
students in each student group will not be uniform. That has the consequence
that a population that evolves really slowly will continue to migrate the same
individual towards the other population. Even if it turned out not to work
well. To counter this, the immigrants compete with the worst individual in
roulette selection. This means that if the immigrants are a lot worse than the
worst individual of this population, the worst individual is likely to remain
in the population.

The idea behind applying the Island model on this task is that what seems to work
extremely well for one student group, might also be close to a good solution in
other student groups. If there are more student groups than two, there are more
migration candidates than slots. In that case, the choice is made using
roulette wheel selection based on the fitness values of each migration
candidate.
\section{Discussion}
\label{sec:approach_discussion}
\subsection{Benefits of the approach}

%\begin{itemize}
%	\item More natural fit to the problem of curriculum sequencing, given the
%		natural mapping of chromosomes and genes to the domain.
%	\item The method is good for changing environments (citation needed)
%	\item No need to explitely specify intermediate states
%	\item Domain allows for many enhancements of the canonical algorithm by
%		adding prior knowledge in the initialization and fitness function, by
%		propagating fitness information to next generations using fitness
%		inheritance, and restrict the crossover and mutation operations to
%		ensure only valid and interesting outcomes.
%	\item Search through the solution space is biased towards areas that seem
%		most promising, with a small but nonzero chance of exploring in other
%		areas.
%	\item This implicit exploration-exploitation trade-off can be enhanced by
%		incorperating uncertainty information
%\end{itemize}
\subsection{Downsides of the approach}
% risk of Hamming walls
\subsection{Comparing with the Markov Decision Process Approach}
%\begin{itemize}
%	\item Paraphrase/quote/link from \citep{Whiteson2012} the following points.
%		(\textbf{Currently copied literally}, better not keep it that way)
%	\item evolutionary methods do not take advantage of the fact the an agent travels
%          to states while performing actions. However evolutionary computation
%		  is still a popular method for solving reinforcement learning
%		  problems. There are three main reasons why.:
%		  \begin{enumerate}
%			  \item ``First, evolutionary methods can cope well with
%					partial observability. While evolutionary methods do not exploit the relationship
%					between subsequent states that an agent visits, this can be advantageous when the
%					agent is unsure about its state. Since temporal-difference methods rely explicitly
%					on the Markov property, their value estimates can diverge when it fails to hold,
%					with potentially catastrophic consequences for the performance of the greedy policy.
%					In contrast, evolutionary methods do not rely on the Markov property and will always
%					select the best policies they can find for the given task. Severe partial
%					observability may place a ceiling on the performance of such policies, but
%					optimization within the given policy space proceeds normally (Moriarty et al, 1999).
%					In addition, representations that use memory to reduce partial observability,
%					such as recurrent neural networks, can be be optimized in a natural way with
%					evolutionary methods (Gomez and Miikkulainen, 1999; Stanley and Miikkulainen, 2002;
%					Gomez and Schmidhuber, 2005a,b).``
%				\item ``Second, evolutionary methods can make it easier to find
%					suitable representations for the agent’s solution. Since policies need only specify
%					an action for each state, instead of the value of each state-action pair, they can be
%					simpler to represent. In addition, it is possible to simultaneously evolve a
%					suitable policy representation``
%				\item ``Third, evolutionary methods provide a simple way to solve
%					problems with large or continuous action spaces. Many temporal-difference
%					methods are ill-suited to such tasks because they require iterating over the action
%					space in each state in order to identify the maximizing action. In contrast,
%					evolutionary methods need only evolve policies that directly map states to actions. Of
%					course, actor-critic methods (Doya, 2000; Peters and Schaal, 2008) and other techniques
%					(Gaskett et al, 1999; Mill ́an et al, 2002; van Hasselt and Wiering, 2007) can
%					also be used to make temporal-difference methods suitable for continuous action
%					spaces. Nonetheless, evolutionary methods provide a simple, effective way to
%					address such difficulties.``
%				\item \citet{Whiteson2012} further mentions: ``Of course, none
%					of these arguments are unique to evolutionary methods, but
%					apply in principle to other policy-search methods too.
%					However, evolutionary methods have proven a particularly
%					popular way to search policy space and, consequently, there
%					is a rich collection of algorithms and results for the
%					reinforcement-learning setting. Furthermore, as modern
%					methods, such as distribution-based approaches, depart
%					further from the original genetic algorithms, their
%					resemblance to the process of natural selection has
%					decreased. Thus, the distinction between evolutionary
%					methods and other policy search approaches has become
%					fuzzier and less important.''
%		  \end{enumerate}
%\end{itemize}
\subsection{Comparing with the Knowledge Engineering Approach}
