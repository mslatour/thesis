%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter: Simulations %
%%%%%%%%%%%%%%%%%%%%%%%%
In order to gain more insight in the properties and effectiveness of
the approach presented in Chapter~\ref{ch:approach} a series of
simulations have been performed for this thesis. These simulations are
based on explicit assumptions regarding the fitness landscape and
facilitate a broader range of analysis than the experiment described in
Chapter~\ref{ch:experimental_setup}. The experiment assesses whether the
analysis based on the simulation results also holds on more realistic data.

This chapter describes the setup of the simulations that have been performed.
The results of these simulations are listed in Chapter~\ref{ch:results}.
%Section listing

\section{General setup}
In order to compare the results from the different simulations more easily, the
termination of the simulation and the manner in which the results are evaluated
are the same for each simulation. This general setup is described in this
section.
\subsection{Termination}
\label{sec:simulations_termination}
As is described in Section~\ref{sec:approach_termination}, the used
implementation of the genetic algorithm is not equiped with termination
conditions given the nature of the domain. However, when multiple simulations
need to be run, this can obviously not be the case. Each simulation setup has
two stopping criteria: \emph{reaching optimality} and \emph{maximum attempts}.
\paragraph{Reaching optimality} An instance of the software is said to have
reached optimality when the sequence that is considered optimal given the
fitness landscape is evaluated twenty consecutive times. In an online course
this would mean that twenty students in a row would be presented the optimal
solution. In the case of multiple sequences being optimal under the fitness
model, each evaluation of any of those sequences is considered an optimal
evaluation.
\paragraph{Maximum attempts} An instance of the software is given a maximum of
2000 evaluations to reach an optimal solution. This could for example mean that
a genetic algorithm could run for 100 generations, where in each generation
there are twenty evaluations for UCB-1 to divide.

\subsection{Evaluation of results}
To aid in the analysis of each simulation result, three views are applied to
the data: \emph{the cumulative regret curve}, \emph{the derivative of the
cumulative regret curve} and \emph{the number of evaluations needed}.
\paragraph{Cumulative Regret Curve} Regret is defined as the difference in
performance between the applied policy and the optimal policy. In the context
of the educational domain, regret is roughly speaking the damage done to the
student's understanding due to the offering of suboptimal educational material.
Due to the use of the UCB-1 selection algorithm, the cumulative regret over time
is mathematically proven to decrease logarithmically. That means that the
cumulative regret curve must resemble the curve of a logarithmic function.
However, this does not mean that the cumulative regret curve does not hold any
information. Due to the combination with the genetic algorithm, the options from
which the UCB-1 algorithm can choose are limited. The interplay between the
genetic algorithm and UCB determines if the cumulative regret converges and the
dimensions of the resulting curve. This curve is especially interesting because
it visualizes the exploration vs. exploitation trade-off.
\paragraph{Derivative of the Cumulative Regret Curve}
The cumulative regret curve can occassionally be difficult to interpret.
When fitness values are noissy or when a lot of mutation occurs one
can expect to find fluctuations in the curve, especially in the beginning. In
order to assist in the interpretation of the curve, the derivative of the curve
is also plotted. The curve of the derivative should at least show a downwards
trend. In the ideal case the curve resembles the derivative of a logarithm.
\paragraph{Number of evaluations needed} In addition to the curves, one could
also look at quantitative results. One of these is the number of evaluations
needed before the tutor reached the optimal solution, as described in
Section~\ref{sec:simulations_termination}. This value cannot be determined in case
the simulation terminated before that point was reached.
\section{Parameters}
The approach taken in this thesis requires several parameters to be set in
advance. These parameters influence the result of the algorithm and are often
not independent. This section describes the effect of each parameters.
\paragraph{Population size} The \emph{population size} refers to the number of individuals in each generation.
When this value is bigger, more diversity can exist in each generation.
The actual amount of diversity is also determined by the diversity of the
previous generation, the selection mechanism and the mutation rate.
What the population size does directly influence is the number of crossover
operations.

\paragraph{Number of episodes} As described in Section~\ref{sec:approach_ucb}, UCB selects which of the
individuals from the current generation are evaluated. UCB gets queried several
times during one generation, how often is determined by the \emph{number of
episodes} parameter. One episode equals one evaluation per generation. When
the value of this parameter is higher, the certainty of the fitness values of
each individual in the generation increases. However, when the individuals
selected are not optimal, a higher value also causes an increase in regret.
\paragraph{Number of elite} Elitism, as described in Section~\ref{sec:approach_generation_switch},
preserves the best performing individuals of the previous generation. The
\emph{number of elite} members corresponds negatively to the number of
crossover operations, since elite members are maintained as is and are not
replaced by their offspring. Note that there is some interplay with
the \emph{population size} in the effect on the number of parents in the
intermediate generation.
\paragraph{Mutation rate} The \emph{mutation rate} determines the chance that a mutation occurs in the
chromosome of a new individual. Mutation ensures that areas of the search space
are explored, even when the genetic algorithm moved in a different direction.
When this parameter value increases, more exploration occurs. Setting this
parameter too high will result in a lack of convergence.
\section{Scenarios}
The simulations run for this thesis demonstrate the effect of these parameters
in various scenarios. First, a handcrafted model scenario will be run where the
sequences are evaluated based on a fitness model manually created by the
author. The fitness model values are based on the author's expectation of the
true value of each sequence of learning material. The handcrafted model for
each population can be found in
Table~\ref{tab:apx_fitness_handcrafted}. Second, the worst-case scenario where
an optimal sequence is surrounded by valley of bad sequences. The purpose of
this scenario is to demonstrate the effects of hamming walls. Third, the
experiment-based scenario where the fitness values observed in the experiment
are converted into a fitness model, with which the sequences in the simulation
are evaluated.
\subsection{Scenario 1: Handcrafted model}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{images/results/plot_cumul_regrets_pop1_xkcd.png}
	\caption{Cumulative regret plot placeholder}
	\label{fig:cumul_placeholder1}
\end{figure}
