% * What are OER
% * What is OER quality? (most features do not involve impact)
% * How is quality assessed (mostly by means of metadata or popularity within
% teacher community)
% * Impact is the big missing factor in automatic quality assessment
% * There are also costs involved in measuring impact
This thesis addresses a problem in the community of \emph{Open Educational
Resources}. It does so with a solution from the \emph{Intelligent Tutoring
Systems} community, called \emph{curriculum sequencing}. More specifically,
the balance between good quality estimates and the cost bad teaching is
optimized as a curriculum sequencing problem. The curriculum sequencing is executed by a genetic algorithm. This chapter
provides a background in OER, automatic assessment of the quality of OER and curriculum sequencing.
Section~\ref{sec:background_oer} describes what OER are. In
Section~\ref{sec:background_quality_assess} an overview is given on how assessment of
quality is done at the moment. Section~\ref{sec:background_curriculum_sequencing} gives an overview of the
relevant literature in the field of curriculum sequencing.

\section{Open Educational Resources}
\label{sec:background_oer}
Several definitions of the term Open Educational Resources (OER) are given, since
the term was first coined by UNESCO in 2002 \citep{Camilleri2014}. UNESCO
defines	OER as follows.
\begin{quote}
``The open provision of educational resources, enabled by
information and communication technologies,
for consultation, use and adaptation by a community of users for non-commercial
purposes'' \citep{UNESCO2002}
\end{quote}
The William and Flora Hewlett Foundation, the key funders of OER initiatives,
define OER as follows.
\begin{quote}
``OER are teaching, learning, and research resources that reside in the public domain or have
been released under an intellectual property license that permits their free
use or re-purposing by others. Open educational resources include full courses,
course materials, modules, textbooks, streaming videos, tests, software, and
any other tools, materials, or techniques used to support access to
knowledge.'' \citep{Atkins2007}.
\end{quote}
OER are closely related to reusable learning objects. The key difference lies
in the emphasis on openness \citep{Camilleri2014}. \citet{Hilton2010} provides a framework to think
about openness in OER, called ``The Four R's of Openness''. The four R's are
reuse, revise, remix and redistribute. In the context of OER, these can be
explained as follows. OER are free to be used in any way (reuse). OER are free
to be altered in any way (revise). OER are free to be combined with other work
(remix). OER are free to be shared with others (redistribute).\\\\
\noindent
\citet{Collis2004} enumerates six stages in the lifecycle of a reusable learning object.
First, a learning object is obtained or created. Second, the learning object is labelled
with metadata information. Third, the learning object is offered, often in a
learning object repository, to be selectable for potential use. Fourth, the
learning object is selected to be used in an educational context. Fifth, the
learning object is used either in a self-contained manner or in combination
with other learning objects in an educational context. Sixth, after of during
the learning object is used a decision is made whether or not to retain this
learning object. Figure~\ref{fig:oer_lifecycle} depicts the cycle that these
stages form.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{images/oer_lifecycle.pdf}
	\caption[Lifecycle of reusable learning objects]{Lifecycle of reusable learning objects, as enumerated by \citep{Collis2004}}
	\label{fig:oer_lifecycle}
\end{figure}\\
\noindent
The advent of OER lowered the threshold of creating and distributing
educational material. \citet{Weller2010} states that the resulting diversity of
authors will cause a less stable level of quality. However, a lot is expected
from these authors. The human instructor plays an important role in almost all
of the mentioned stages in Figure~\ref{fig:oer_lifecycle}. In particular, the
human instructor is responsible in most systems for quality assessment.
According to \citep{Camilleri2014} quality assurance ranges from strict
top-down controlled production processes to peer-review and everything in
between.\\\\
\noindent
However, according to \citep{Ochoa2009a} the number of OER in learning object
repositories vary from hundreds to millions of objects. The growth of
these repositories is still linear in the number of authors \citep{Ochoa2009a}.
However, \citeauthor{Ochoa2009a} expect that this growth will become
exponential when authors are better retained by the repositories. Determining
the quality of an exponentially growing number of open educational resources by
human effort is infeasible \citep{Cechinel2011, Ochoa2006, Zemsky2004}.
As a result, there is a growing interest in automatic assessments of OER
quality.
\section{Automatic assessment of OER Quality}
\label{sec:background_quality_assess}
One approach focuses on the quality of the metadata of the learning
object \citep{Ochoa2009b, Tani2013}, as a proxy for the quality of the object
itself. A problem with this approach is that metadata can be inaccurate
\citep{Cechinel2009} and incomplete \citep{Sicilia2005}. A different approach is
to automatically assess the quality of learning objects based on intrinsic
metrics, such as the number of links in a learning object. \citet{Cechinel2011}
managed to find statistical profiles of different quality labels by comparing
intrinsic metrics of good and poor learning objects in the MERLOT repository.
\citet{Duval2006} however states that the quality of a learning object is
context-dependent and intrinsically subjective. As examples of context Duval
mentions a.o. learning goal, available time and educational level. Duval
proposes the context-dependent ranking algorithm LearnRank, in which learning
objects that are used in many contexts receive a higher rank. In
\citep{Ochoa2006} the ``use of contextualized attention metadata for ranking and
recommending learning objects'' is proposed, where the attention given to a
learning object by a student or a teacher within a certain context is
considered to be a proxy for the usefulness of the object in that context.
Duval however also mentions in \citep{Duval2006} that ``In an ideal world, we
would actually bootstrap and steer this process [of establishing the LearnRank]
through empirical data on the learning effect that specific objects have
actually caused (or helped to realise) in specific contexts ...''. According
to \citep{Kay2007} the effect of a learning object on ’actual learning’ is
underrepresented in the research on learning objects.\\\\
\noindent
In a recent report from the European Commission, quality of Open Educational
Resources had five aspects: efficacy, impact, availability, accuracy and
excellence \citep{Camilleri2014}. The aspect of impact referred to the “extend
to which an object or concept proves effective”. However, little is said in
\citet{Camilleri2014} about methods that would measure this impact.
\citeauthor{Camilleri2014} further state as one of the challenges that
educational resources of high quality are fragmented with no particular way of
distinguishing them from the other available learning objects. They recommend
creating “specialised directories” where exclusive lists of repositories of
high-quality content are maintained. It is however not stated how these
high-quality repositories could incorporate impact in their quality
assessment.\\\\
\noindent
This thesis presents work that assesses OER quality by the impact it has on
learning. This impact is measured by assessing the competence level before and
after the OER is presented. The normalized learning gain (NLG) is used to express the
impact the OER has on the competence level. The NLG metric is a widely adopted
measure of student learning. The main advantage of NLG is that student learning
is measured irrespective of the student's incoming competence \citep{Chi2010}.
The NLG metric is calculated using formula \eqref{eq:nlg}, where $C^1$ and
$C^2$ denote the pre-test and post-test scores respectively. The value $1$ is
assumed to be the maximum score of an assessment. Table~\ref{tab:nlg_values}
shows the NLG value for a variety of parameter values. The normalized learning
gain is essentially a normalized distance metric. Thus, any linear function
expressing competence with a known maximum value is applicable to $C^1$ and
$C^2$. The experiments performed for this thesis use the ratio of correct
answers. Using the NLG metric, sequences of OER can be evaluated by
comparison of the assessment score before and after presentation.
\begin{equation}
	nlg(C^1, C^2) = \frac{C^2-C^1}{1-C^1}
	\label{eq:nlg}
\end{equation}
\begin{table}
	\caption{Different NLG values}
	\label{tab:nlg_values}
	\begin{subtable}{0.25\linewidth}
	\centering
	\begin{tabular}{lll}\hline
		$\mathbf{C^1}$ & $\mathbf{C^2}$ & \textbf{NLG} \\\hline
		0		& 0			& 0 \\
		0		& $^1/_3$	& $^1/_3$\\
		0		& $^2/_3$	& $^2/_3$\\
		0		& 1			& 1\\
	\end{tabular}
	\end{subtable}
	\hspace{0.1\linewidth}
	\begin{subtable}{0.25\linewidth}
	\centering
	\begin{tabular}{lll}\hline
		$\mathbf{C^1}$ & $\mathbf{C^2}$ & \textbf{NLG} \\\hline
		$^1/_3$	& 0			& $-^1/_2$\\
		$^1/_3$	& $^1/_3$	& 0\\
		$^1/_3$	& $^2/_3$	& $^1/_2$\\
		$^1/_3$	& 1			& 1\\
	\end{tabular}
	\end{subtable}
	\hspace{0.1\linewidth}
	\begin{subtable}{0.25\linewidth}
	\centering
	\begin{tabular}{lll}\hline
		$\mathbf{C^1}$ & $\mathbf{C^2}$ & \textbf{NLG} \\\hline
		$^2/_3$	& 0			& $-2$\\
		$^2/_3$	& $^1/_3$	& $-1$\\
		$^2/_3$	& $^2/_3$	& 0\\
		$^2/_3$	& 1			& 1\\
	\end{tabular}
	\end{subtable}
\end{table}
\section{Curriculum Sequencing}
\label{sec:background_curriculum_sequencing}
Curriculum sequencing concerns generating a sequence of teaching operations
that is optimal for an individual learning \citep{Brusilovsky2003a}. The level
on which this takes place ranges from course sequencing to content sequencing.
Curriculum sequencing is an established part of the larger field of intelligent
tutoring systems \citep{Brusilovsky1996}. \citet{Vanlehn2006} provides an
introduction into intelligent tutoring systems. Although most intelligent
tutoring systems used to be supported by extensive and explicit knowledge
engineering, data-driven systems are now emerging \citep{Koedinger2013}. Within
and around the intelligent tutotoring systems community there have been
different technologies involved with curriculum sequencing tasks.\\\\
\noindent
The field of Adaptive Hypermedia Systems (AHS) explores adaptive presentation and
adaptive navigation in hypertext documents \citep{Brusilovsky2003b}. Many
existing AHS require the set of documents to be known in advance, referred to
as a \emph{closed corpus} \citep{Brusilovsky2007}. According to
\citeauthor{Brusilovsky2007}, these documents are annotated with metadata and
linked to ontologies before presented to students. Based on this additional
information adaptation takes place to cater for individual needs. Examples of
such systems \citep{Aerts2002, Kravcik2004} are used in formal education as
authoring tool. Due to the \emph{closed corpus} assumption, this systems are
not useful in dealing with open educational resources \citep{Brusilovsky2007}.
\citeauthor{Brusilovsky2007} further state that AHS should move to work with
an \emph{open corpus}.\\\\
\noindent
A technique that is much more suited for the open corpus task is
\emph{collaborative filtering} \citep{Manouselis2011}. Together with
\emph{content-based filtering} and \emph{hybrid models}, it is one of the three
main recommendation techniques \citep{Verbert2012}. Collaborative filtering
bases recommendations on actions of similar people. Content-based filtering
compares content instead of people and bases recommendations on that. Hybrid
models combine multiple recommender systems. \citep{Manouselis2011} provides an
extensive review of recommender systems using in technology-enhanced learning.
A recent example that was published after the review is \citep{Verbert2012}.
\citet{Verbert2012} recommends learning designs to teachers based on patterns
of existing learning designs of peers. Furthermore, resources are recommended
within these designs, based on students' usage data.\\\\
\noindent
A Markov Decision Processes are a branch of reinforcement learners that
indirectly generate sequences. \citep{Chi2010} presents a conversational
physics tutor that makes micro-decisions about whether to tell or elicit a
certain fact. Features from both the conversation and the student performance
are used. As a result, a sequence of pedagogical actions is created.\\\\
\noindent
Another popular approach to curriculum sequencing is evolutionary computing.
\citet{AlMuhaideb2011} provides an extensive overview of this literature.
The main approach is taking curriculum sequencing to be a constraint
problem. Several systems include a term that expresses how
well a particular solution fits the pre-determined prerequisite structure of
the learning objects~\citep{Seki2005, Chen2009, Samia2007}.
Other work in this field includes a term that expresses how smooth the
transitions are between learning objects in difficulty~\citep{Hovakimyan2004,
Seki2005, Chen2008, Huang2007} or how well their difficulty matches the
compentency level of the student \citep{Seki2005, Chen2008, Chen2009,
Samia2007, Huang2007}. \citet{deMarcos2009, deMarcos2007} redefines the
sequencing problem to be a permutation problem and applies both a genetic
algorithm and a particle swarm optimization. The sequencing done tries to
match compentencies the students has or desires with compentency-related
metadata of the learning objects. Both evolutionary algorithms work, but
particle swarm optimization outperforms the genetic algorithm.\\\\
\noindent
The proposed work in this thesis applies the genetic algorithm with many of the
same features as ealier systems, such as generational replacement,
integer-encoding for chromosomes and elitism. However, unlike all mentioned work, the approach taken in this thesis
uses the normalized learning gain caused by the sequence to be its fitness.
Furthermore, the learning objects are treated as black boxes in this thesis. No
metadata or expert-driven ontologies are used. The only thing known about the
learning objects, within the context of this thesis, is their topic. 
